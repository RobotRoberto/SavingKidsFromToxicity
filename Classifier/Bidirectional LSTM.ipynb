{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras with Tensorflow backend\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property and everything I have! I wish they were strict facts do you know what stupid Canadians do? Nothing. They speak French, and play in traffic. Are you gay son of a philosophical paradigm which is not perminently banned!\n"
     ]
    }
   ],
   "source": [
    "train = pandas.read_csv('../dataset/train.csv')\n",
    "test = pandas.read_csv('generated_phrases.csv', encoding=\"latin-1\")\n",
    "\n",
    "toxic_classifications = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[toxic_classifications].values\n",
    "train_sentences_strings = train[\"comment_text\"]\n",
    "test_sentences_strings = test[\"text\"]\n",
    "\n",
    "print(test_sentences_strings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 12052, 1116, 86, 331, 51, 2278, 11448, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 15190, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n"
     ]
    }
   ],
   "source": [
    "maxfeatures = 30000\n",
    "tokenizer = Tokenizer(num_words = maxfeatures)\n",
    "tokenizer.fit_on_texts(list(train_sentences_strings))\n",
    "tokenized_training = tokenizer.texts_to_sequences(train_sentences_strings)\n",
    "tokenized_testing = tokenizer.texts_to_sequences(test_sentences_strings)\n",
    "\n",
    "print(tokenized_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding for Apropriate Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddingSize = 300\n",
    "\n",
    "padded_train = pad_sequences(tokenized_training, maxlen=paddingSize)\n",
    "padded_test = pad_sequences(tokenized_testing, maxlen=paddingSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 100)     3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 300, 100)     80400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 298, 64)      19264       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            774         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,100,438\n",
      "Trainable params: 3,100,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "length = 300\n",
    "\n",
    "inputLayer = Input(shape = (length, ))\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "x = Embedding(maxfeatures, embedding_size) (inputLayer)\n",
    "\n",
    "#begin LSTM layer\n",
    "lstm_output_size = 100\n",
    "x = LSTM(lstm_output_size, return_sequences = True, name = 'lstm')(x)\n",
    "\n",
    "#lstm reshaping with GLOBAl pooling\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "\n",
    "#dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "#dense\n",
    "x = Dense(6,activation = \"sigmoid\")(x)\n",
    "\n",
    "lstmModel = Model(inputs = inputLayer, outputs = x)\n",
    "\n",
    "lstmModel.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(lstmModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs = 3\n",
    "Batch = 32\n",
    "\n",
    "Lucas : 1 dense layer, 2 dense layers, sigmoid vs relu\n",
    "Robert : Bidirectional stuff\n",
    "Nick : layer dimensions / output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_train, y, train_size=0.9)\n",
    "\n",
    "filepath=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "roc_auc = RocAucEvaluation(validation_data=(X_test, y_test), interval = 1)\n",
    "callbacks_list = [roc_auc,checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "lstmModel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmModel.save(\"Bidirectional_LSTM_Convolution.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"Bidirectional_LSTM_Convolution.hdf5\"\n",
    "lstmModel.load_weights(filename)\n",
    "lstmModel.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstmModel.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "test.to_csv(\"generated_text_classification.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM with 2 dense/dropout layers: loss: 0.0444 - acc: 0.9833 - val_loss: 0.0475 - val_acc: 0.9822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM with 1 dense/dropout layer: loss: 0.0535 - acc: 0.9817 - val_loss: 0.0486 - val_acc: 0.9820"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM-Convolution: loss: 0.0431 - acc: 0.9837 - val_loss: 0.0451 - val_acc: 0.9831 => ROC-AUC - epoch: 2 - score: 0.983753"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
