{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras with Tensorflow backend\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\r\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "train = pandas.read_csv('dataset/train.csv')\n",
    "test = pandas.read_csv('dataset/test.csv')\n",
    "\n",
    "toxic_classifications = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[toxic_classifications].values\n",
    "train_sentences_strings = train[\"comment_text\"]\n",
    "test_sentences_strings = test[\"comment_text\"]\n",
    "\n",
    "print(train_sentences_strings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27904, 75, 1, 128, 130, 177, 28, 670, 4510, 12133, 1115, 87, 331, 52, 2269, 11510, 51, 6926, 16, 61, 2747, 149, 8, 2933, 35, 116, 1235, 15747, 2859, 5, 46, 60, 243, 1, 370, 32, 1, 39, 29, 144, 74, 3474, 90, 3075, 4630, 2284, 985]\n"
     ]
    }
   ],
   "source": [
    "maxfeatures = 30000\n",
    "tokenizer = Tokenizer(num_words = maxfeatures)\n",
    "tokenizer.fit_on_texts(list(train_sentences_strings))\n",
    "tokenized_training = tokenizer.texts_to_sequences(train_sentences_strings)\n",
    "tokenized_testing = tokenizer.texts_to_sequences(test_sentences_strings)\n",
    "\n",
    "print(tokenized_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding for Apropriate Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddingSize = 300\n",
    "\n",
    "padded_train = pad_sequences(tokenized_training, maxlen=paddingSize)\n",
    "padded_test = pad_sequences(tokenized_testing, maxlen=paddingSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 300, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 300, 100)          80400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 3,085,756\n",
      "Trainable params: 3,085,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "length = 300\n",
    "\n",
    "inputLayer = Input(shape = (length, ))\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "x = Embedding(maxfeatures, embedding_size) (inputLayer)\n",
    "\n",
    "#begin LSTM layer\n",
    "lstm_output_size = 100\n",
    "x = LSTM(lstm_output_size, return_sequences = True, name = 'lstm')(x)\n",
    "\n",
    "#lstm reshaping with GLOBAl pooling\n",
    "x = GlobalAveragePooling1D(x)\n",
    "\n",
    "#dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "#dense\n",
    "x = Dense(50, activation = \"relu\")(x)\n",
    "\n",
    "#dropout layer\n",
    "x = Dropout(0.05)(x)\n",
    "\n",
    "#dense\n",
    "x = Dense(6,activation = \"sigmoid\")(x)\n",
    "\n",
    "lstmModel = Model(inputs = inputLayer, outputs = x)\n",
    "\n",
    "lstmModel.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(lstmModel.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs = 3\n",
    "Batch = 32\n",
    "\n",
    "Lucas : 1 dense layer, 2 dense layers, sigmoid vs relu\n",
    "Robert : Bidirectional stuff\n",
    "Nick : layer dimensions / output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 931s 6ms/step - loss: 0.0699 - acc: 0.9774 - val_loss: 0.0491 - val_acc: 0.9818\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 951s 7ms/step - loss: 0.0444 - acc: 0.9833 - val_loss: 0.0475 - val_acc: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20205389978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "lstmModel.fit(padded_train,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 143613 samples, validate on 15958 samples\n",
    "Epoch 1/2\n",
    "loss: 0.1032 - acc: 0.9688 - val_loss: 0.0654 - val_acc: 0.9767\n",
    "Epoch 2/2\n",
    "loss: 0.0574 - acc: 0.9797 - val_loss: 0.0547 - val_acc: 0.9804\n",
    "\n",
    "lstm_output_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 143613 samples, validate on 15958 samples\n",
    "Epoch 1/2\n",
    "loss: 0.0934 - acc: 0.9714 - val_loss: 0.0583 - val_acc: 0.9789\n",
    "Epoch 2/2\n",
    "loss: 0.0520 - acc: 0.9810 - val_loss: 0.0527 - val_acc: 0.9804\n",
    "\n",
    "relu dense layer 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 143613 samples, validate on 15958 samples\n",
    "Epoch 1/2\n",
    "loss: 0.0699 - acc: 0.9774 - val_loss: 0.0491 - val_acc: 0.9818\n",
    "Epoch 2/2\n",
    "loss: 0.0444 - acc: 0.9833 - val_loss: 0.0475 - val_acc: 0.9822\n",
    "\n",
    "GlobalMaxPool1D()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
